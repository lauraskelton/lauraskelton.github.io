<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Objective-C | Hacker School Daily Log]]></title>
  <link href="http://lauraskelton.github.io/blog/categories/objective-c/atom.xml" rel="self"/>
  <link href="http://lauraskelton.github.io/"/>
  <updated>2014-06-17T11:44:59-04:00</updated>
  <id>http://lauraskelton.github.io/</id>
  <author>
    <name><![CDATA[Laura Skelton]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hacker School Day 5]]></title>
    <link href="http://lauraskelton.github.io/blog/2014/06/16/hacker-school-day-5/"/>
    <updated>2014-06-16T21:04:29-04:00</updated>
    <id>http://lauraskelton.github.io/blog/2014/06/16/hacker-school-day-5</id>
    <content type="html"><![CDATA[<p>I continued working through the <a href="http://neuralnetworksanddeeplearning.com">Neural Networks book</a>. A lot of the rusty math started finally coming back to me in sudden bursts of insight, and the book broke the main formulas down enough that I was able to really follow what was going on and how the pieces were working together on a conceptual and mathematical level.</p>

<p>I got some Python code running in the terminal to create and backpropagate a neural network for classifying handwritten digits, using the MNIST dataset, following along with the exercises in the book. It&rsquo;s pretty exciting how quickly the error decreases with the stochastic gradient descent optimization on the weights and biases of the neural network nodes. It seems to learn very quickly!</p>

<p><img class="center" src="/images/posts/5digits.png" title="&lsquo;handwritten digits&rsquo;" ></p>

<p>I was bummed to learn that deep neural networks, which have multiple hidden layers between the input and output nodes, are apparently much harder to train than these &ldquo;shallow&rdquo; neural networks with just one hidden layer. Deeper networks are awesome because they can make much smarter decisions and much more complicated distinctions and abstractions between things by looking at patterns in a more complicated way. There is apparently some very recent math (from 2006!) that enables more efficient learning in deeper neural nets, that can train networks with 5 to 10 hidden layers. I&rsquo;m looking forward to learning how to implement these deeper networks, but will have to find a new source of learning materials as this book is currently unfinished and stops after explaining the mathematical proof and conceptual model behind back propagation.</p>

<p><img class="center" src="/images/posts/5deepnetworklayer.png" title="&lsquo;Deep Neural Network&rsquo;" ></p>

<p>What is cool about the limitations of training deep neural networks is that one of my early ideas for a Hacker School project now seems like it would actually be useful, more than when I mistakenly believed that many-layered neural networks were easy to set up and train. I wanted to make a modular neural network for image classification, with input layers that took particular sets of images and output them into different categories. For example, networks that detect lines, or shapes, or an eye, or sort by color, etc. The neural networks could be set up in such a way that they are trained individually, but they could be linked to feed information from one to the next in order to do more complicated classifications. Training neural networks takes some processing power, but once they are well-trained, they can process and classify new data very quickly, even running in the browser. I think it would be a really cool open-source project if I could set up a modular framework that people could add different types of pre-trained visual image processing neural networks that could then be linked up in a custom way for anyone who needed to do image classification quickly.</p>

<p><img class="center" src="/images/posts/5deepnetwork.png" title="&lsquo;Deep Neural Network Image Classification&rsquo;" ></p>

<p>I discovered <a href="http://evolvingstuff.blogspot.com/2012/12/learning-mnist-with-shallow-neural.html">another article about shallow neural networks and handwritten digit classification</a> that was very exciting! I hadn&rsquo;t realized until I read it that once the neural network is trained, you could display the relative weights of the inputs to each hidden neuron as a representation of what that hidden neuron was looking for. This is a very similar concept to the discovery of independent features in Non-Negative Matrix Factorization that I learned about last week. The &ldquo;features&rdquo;, or the features the hidden nodes represent, can be represented and described purely by the relative weights of the data inputs they represent. For the articles and topic themes in the Non-Negative Matrix Factorization exercise, that would be the words that describe the theme. For handwritten digit classification, that turns out to be the intensity of the weight of each pixel input for that node, which can be represented visually.</p>

<p><img class="center" src="/images/posts/5input1.jpg" title="&lsquo;handwritten digit feature&rsquo;" ></p>

<p><img class="center" src="/images/posts/5input2.jpg" title="&lsquo;handwritten digit feature&rsquo;" ></p>

<p><img class="center" src="/images/posts/5input3.jpg" title="&lsquo;handwritten digit feature&rsquo;" ></p>

<p><img class="center" src="/images/posts/5input4.jpg" title="&lsquo;handwritten digit feature&rsquo;" ></p>

<p><img class="center" src="/images/posts/5input5.jpg" title="&lsquo;handwritten digit feature&rsquo;" ></p>

<p>You can actually see what visual features of the image a particular hidden node of the digit classifier neural network is looking for, and it&rsquo;s easy to imagine that if two hidden nodes were highly active, it could represent for example a top curve and a bottom curve that would indicate the number zero. This visualization of the input weights helped a lot with my conceptual understanding of what the hidden nodes represent and how they might be working together.</p>

<p>This got me very excited about the idea of applying a neural network classifier to my beer ratings dataset of 200,000 beer ratings from a few thousand users of a couple thousand beers. I realized that not only can I use a neural network to recommend beers to users very quickly once the network is trained, but I realized that for beers, the hidden nodes would represent groupings of beers into a sort of taste profile. I&rsquo;m imagining a node each for hoppy-bitter beers, boozy belgians, roasty stouts, and sweeter lighter beers, with each individual user&rsquo;s preferences being some combination of those flavor types, represented in the program by different activations of those hidden nodes. This is very exciting!</p>

<p>I got my beer data into an appropriate format and trained a neural network with 10 hidden nodes (which to my understanding represents 10 beer-taste-profiles), and tomorrow I&rsquo;m going to work on a representation showing which of the top 100 beers activate each node, representing independent features (meaningful beer groupings) of the dataset.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hacker School Day 3]]></title>
    <link href="http://lauraskelton.github.io/blog/2014/06/12/hacker-school-day-3/"/>
    <updated>2014-06-12T20:55:09-04:00</updated>
    <id>http://lauraskelton.github.io/blog/2014/06/12/hacker-school-day-3</id>
    <content type="html"><![CDATA[<p>I started working on fixing some of the bugginess of the Multipeer Connectivity framework in ObjC in the morning. I tried to hack it so that Multipeer would continue working when the app was in background mode (not currently possible due to Apple&rsquo;s imposed limitations), so that we could have a count of how many people were connected to the network.</p>

<p>I also spent some time working on a problem that seems to be coming from issues with lower-level Bluetooth LE frameworks, where there are errors when trying to reconnect to the same peer multiple times after disconnecting. The issues are linked, because there can be problems rejoining the network after the backgrounded app is opened again and tries to reconnect. I suspect it has to do with sessions not fully closing from the first session when the device attempts to start a new session.</p>

<p>I got a much deeper understanding of the capabilities of Multipeer while I was walking through the code handling the sessions and peers within the network, but in the end I concluded that Multipeer is still pretty buggy. I&rsquo;m reconsidering starting with our own implementation of a mesh network in Bluetooth LE instead of using Multipeer, since it seems like Multipeer doesn&rsquo;t really allow us to avoid some of the tricky issues we were handling in CoreBluetooth, and possibly the higher-level framework gives us less power to debug and work around some of the problems. CoreBluetooth also possibly avoids some of the issues with Multipeer not working in background mode, since we can set a Bluetooth LE message to broadcast in the background, and detecting a nearby iBeacon should be able to &ldquo;wake up&rdquo; an app even if it has been closed by the user. Plus I&rsquo;ve already played around with CoreBluetooth with my <a href="https://github.com/lauraskelton/simple-share">SimpleShare</a> Bluetooth LE Local Sharing project.</p>

<p>I switched tasks to continue working through <a href="http://shop.oreilly.com/product/9780596529321.do">Programming Collective Intelligence</a>, an amazing book on Machine Learning algorithms. It was wonderful to get into working on Machine Learning again, since it&rsquo;s what I had originally intended to focus on at Hacker School before I was swept up with Swift.</p>

<p><img class="center" src="/images/posts/pci.jpg" title="&lsquo;programming collective intelligence&rsquo; &lsquo;programming collective intelligence&rsquo;" ></p>

<p>I worked through the chapter on Non-Negative Matrix Factorization today, which was actually way more awesome than it sounds. You can scan through a bunch of articles and get word counts, just like in an earlier chapter on document classification using a naive Bayesian classifier. The Bayesian classifier used a weighted probability based on the word counts to predict which category a document belonged to out of a set of predetermined category options, and based on initial training data.</p>

<p>Non-Negative Matrix Factorization is WAY cooler! It starts with the same word-count information for each article set up in a matrix, then factors the matrix into two matrices that, multiplied together, would be approximately equal to the original matrix (articles vs. words). The two factor matrices give you some really neat information about the data set, and they actually extract what the features (categories/themes) are automatically given a set number of desired features. The features can be described by which words are most likely to appear in them. So the first factor matrix has a column for each word and a row for each feature/theme/category, with each number in the matrix being the likelihood that a particular word belongs to that theme, and the second factor matrix has a row for each article and a column for each feature, and the numbers in the matrix correspond to the weight of each theme in that article.</p>

<p><img class="center" src="/images/posts/nmf1.jpg" title="&lsquo;non-negative matrix factorization&rsquo;" ></p>

<p><img class="center" src="/images/posts/nmf2.jpg" title="&lsquo;non-negative matrix factorization&rsquo;" ></p>

<p><img class="center" src="/images/posts/nmf.jpg" title="&lsquo;non-negative matrix factorization&rsquo;" ></p>

<p>It&rsquo;s very cool to see how some clever math and optimization can actually allow the algorithm to come up with its own categories/classes of articles (or anything else you&rsquo;re studying), without having any training data or input from the user about what is expected.</p>

<p>I have one more chapter in the book I&rsquo;ve been working through that I&rsquo;m excited to get through tomorrow, on Genetic Programming, which is all about writing programs that can actually modify themselves and evolve the code itself. Super cool!</p>

<p>I&rsquo;m also looking forward to getting started on learning about Neural Networks more in-depth. I&rsquo;d really like to understand on a deep conceptual level what neural nets are really doing, and this online book looks like a great way to get started. <a href="http://neuralnetworksanddeeplearning.com">http://neuralnetworksanddeeplearning.com</a></p>

<p>I ended the day by doing a short presentation about a beer clustering project I did a couple of months ago in Python. My beer recommendations iOS app generated over 200,000 beer ratings, which has been a pretty cool data set to play around with. I started by scattering beers randomly in a 2 dimensional grid, then moved the points to try to get the distances between each beer to approximate the Pearson distance I calculated by comparing user ratings of all of the beers to find which were most similar (and so should be closer together on the beer map), and which beers were most different, and so should be far apart. I figured out some tricks to break out of the local minimums I kept running into, and ended up with a <a href="http://beerchooser.com/beermap/beergmap.html">pretty cool map</a> of many different beers and how they are related by user preference to each other. (The green color represents IBU &ndash; International Bittering Units, a measure of bitterness/hoppiness in beer, and the blue color represents ABV, or alcohol content, of each beer.)</p>

<p>It was my first time ever presenting a coding project I&rsquo;d done in front of a group of people, and it was a cool experience to share something I&rsquo;d been working on with a group that could follow what I&rsquo;d done and understand what was neat about it. It was great practice for me in talking about my coding work to a larger group, which I have done much more with my design work than with my programming projects.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hacker School Day 2]]></title>
    <link href="http://lauraskelton.github.io/blog/2014/06/11/hacker-school-day-2/"/>
    <updated>2014-06-11T20:44:53-04:00</updated>
    <id>http://lauraskelton.github.io/blog/2014/06/11/hacker-school-day-2</id>
    <content type="html"><![CDATA[<p>I had such a fun day! In the morning I learned how to use Arduino to control Neopixel RGB LED lights from Adafruit. It was so fast to get going with this, and the code was easy to tweak to change the patterns the lights would display.</p>

<p><img class="center" src="/images/posts/adapixels.jpg" title="&lsquo;adafruit neopixels&rsquo; &lsquo;adafruit neopixels&rsquo;" ></p>

<p>We wrote some code a little later that cycled through a rainbow with alternating pixels flashing opposite colors off and on.</p>

<p>```java
void rainbow(uint8_t wait) {
  uint16_t i, j;</p>

<p>  for(j=0; j&lt;256; j++) {</p>

<pre><code>for(i=0; i&lt;strip.numPixels(); i++) {
  if (i%2==0) {
      if ((j/10) %2==0) {
        strip.setPixelColor(i, Wheel((255-i-j) &amp; 255));
      } else {
        strip.setPixelColor(i,0,0,0);
      }
  } else {
    if ((j/10) %2!=0) {
         strip.setPixelColor(i, Wheel((i+j) &amp; 255));
      } else {
        strip.setPixelColor(i,0,0,0);
      }
  }
}
strip.show();
delay(wait);
</code></pre>

<p>  }
}
```</p>

<p><code>Wheel</code> is a neat little function in the Adafruit Neopixel code that converts a value from 0 to 255 to an RGB color value.</p>

<p><code>java
// Input a value 0 to 255 to get a color value.
// The colours are a transition r - g - b - back to r.
uint32_t Wheel(byte WheelPos) {
  if(WheelPos &lt; 85) {
   return strip.Color(WheelPos * 3, 255 - WheelPos * 3, 0);
  } else if(WheelPos &lt; 170) {
   WheelPos -= 85;
   return strip.Color(255 - WheelPos * 3, 0, WheelPos * 3);
  } else {
   WheelPos -= 170;
   return strip.Color(0, WheelPos * 3, 255 - WheelPos * 3);
  }
}
</code></p>

<p>I&rsquo;m so excited to learn more about hardware and connecting apps to display patterns with the lights.</p>

<p>I spent some time brainstorming about Mesh Networking with a group. We want to create our own implementation of ICMP (Internet Control Message Protocol) over Bluetooth LE, using iBeacons as a sort of push notification to alert nearby devices that messages are available, and broadcasting messages through the nodes of the network to get to the intended user.</p>

<p>We&rsquo;re planning to link the mesh network up with a hardware project using the programmable RGB LED lights from Adafruit to represent the users of the network. I thought it would be cool to make a sort of giant firefly jar with light-dots flying around inside, with one dot representing each user-node of the mesh network. The ObjC iOS app would allow users to join the network and then set their dot&rsquo;s color, which would be reflected in the physical firefly jar.</p>

<p>We found a source for an affordable Bluetooth LE adapter for the Arduino and mapped out a plan for how to design the network on the whiteboard. Excited to give it a go tomorrow!</p>
]]></content>
  </entry>
  
</feed>
